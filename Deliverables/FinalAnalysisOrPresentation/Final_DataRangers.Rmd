---
title: "Data Rangers - Crime in NE"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Research Data Sets
Our project includes the use of 3 research questions, each paired with a distinct data set:

1. How does the population level of a county impact the type of crimes committed?
  * County Population and Crime
2. How do police staffing levels impact overall crime rates?
  * Law Enforcement Staffing Rates
3. How do the types and amounts of crime committed change over the course of a year?
  * Unemployment Level and Crime

Portions of this report are proceeded by the above titles to indicate which data and research questions they relate to.

## Data Cleaning

**Licensing Information:**

All Nebraska data analyzed in this report falls under a U.S. Government Works license, meaning &quot;...there is generally no copyright restriction on reproduction, derivative works, distribution, performance, or display of a government work.&quot; ( [https://www.usa.gov/government-works](https://www.usa.gov/government-works))

No licensing information was provided for the S&amp;P 500 data obtained from [http://www.multpl.com/](http://www.multpl.com/), their site only states &quot;Information is provided &#39;as is&#39; and solely for informational purposes, not for trading purposes or advice, and may be delayed.&quot;.

The Nebraska unemployment data in this report is fetched from Bureau of Labor Statistics ( [https://data.bls.gov/pdq/SurveyOutputServlet](https://data.bls.gov/pdq/SurveyOutputServlet)) and according to their website policy, the public has a right to access the Bureau&#39;s information and can also request access to federal agency records and obtain information.

The offence data statistics was obtained from the Nebraska government database ( [http://www.nebraska.gov/crime\_commission/arrest/offense.cgi](http://www.nebraska.gov/crime_commission/arrest/offense.cgi)) and according to their information use policy, the public can access, copy or download the material available on their website.

**Research Questions:**

All research questions concern the years 2007-2016 in the State of Nebraska.

  **1.** How does a county&#39;s population level impact the types of crimes committed?  
  **2.** How do police staffing levels impact overall Nebraska crime rates?  
  **3.** What trends in the types and amount of crimes committed can be observed?  

**Data Description:**

**General Methodology**

This project started when it was discovered the State of Nebraska provides open data querying for numerous government agencies.  Among them, the Nebraska Crime Commission (NCC) collects information about arrests, police staffing levels, and geographic crime trends.  Through the NCC site (https://ncc.nebraska.gov/stat-reports#Crime\_in\_Nebraska\_Series), we were able to access aggregated data about crime rates going back to 2007.  To complement this data, additional tables were scraped from queries executed on the NCC Arrest Data Query page ( [http://www.nebraska.gov/crime\_commission/arrest/arrest.cgi](http://www.nebraska.gov/crime_commission/arrest/arrest.cgi)).

Within the Arrest Data Query page, there were many options of queries to run.  We decided to first define our research questions outlined above, and then scrape the data most helpful to our analysis. Each team member was assigned a question to find, clean, and combine appropriate data for. We will now look at each of these raw data sources in more detail.

**Raw Data Sources:**

**Law Enforcement Employment in Nebraska Series, 2016-2007**

Citation:

&quot;Law Enforcement Employment in Nebraska Series.&quot; 2017, ncc.nebraska.gov/stat-reports#Crime\_in\_Nebraska\_Series. Accessed 1 Oct. 2017.

Description:

This data is available as a series of PDFs for the years 2016 to 2001.  We are limiting the scope of our research from 2016-2007.  Each PDF is 9 pages long, and features data on the full-time, part-time, and understaffing status for each department, for both sworn officer and civilian employees.  The main section of each report is an 80 row, 14 column, 2 page table, detailing the aforementioned statistics for each Nebraska police agency.

There are additional derived tables from this main section, organizing the statistics by cities above and below 5,000 population, and which agencies are state agencies. We chose to analyze the table found on page 4 of the report, agencies who serve cities of above 5,000 people. This data will be combined with crime data for the counties by year, to understand how agency staffing impacts crime levels within the counties they patrol.  A sample list of header names would be:

- **●** Department
- **●** Full- Time Employees
  - **○** Sworn Officers
    - **■** Male
    - **■** Female
  - **○** Civilians
    - **■** Male
    - **■** Female
- **●** Part-Time Employees
  - **○** Sworn Officers
  - **○** Civilians
- **●** Understaffing
  - **○** Sworn
    - **■** Full-Time
    - **■** Part-Time
  - **○** Civilian
    - **■** Full-Time
    - **■** Part-Time
- **●** Population Covered
- **●** Officers Per 1,000 Population

Metadata:

As this is a published report and not a table in a database, all row and column titles were clear to understand. There is a textual description at the top of the report detailing where the information was collected from and how to interpret the results.  At the end of the report, contracting information for each agency is provided. Google can be used to discover more information about specific agencies.

Remediation Needed:

The overarching challenge was converting the data from PDF to an analysis ready format. Once we identified the portion of the report we decided to scrape, further interrogation reveals columns where every value is 0, leading to questions of whether it should be included, and quadruple row header names would be present in our final table. Department names varied across years, and departments were created and disbanded, but the table remained in the same shape.

**Crime in Nebraska Series, 2016-2007**

Citation:

&quot;Crime in Nebraska Series.&quot; 2017, ncc.nebraska.gov/stat-reports#Crime\_in\_Nebraska\_Series. Accessed 1 Oct. 2017.

Description:

This data is available as a series of PDFs for the years 2016 to 2001.  We are limiting the scope of our research from 2016-2007.  Each PDF is 5 pages long, featuring the following table titles of aggregated crime information for the year.:

- **●** Crime Index Offenses, comparing current to previous year
- **●** Percent Change - Crime Index Offenses, January thru December, 2015 - 2016 by Population Groups
- **●** Total Arrest Trends, 2015 - 2016
- **●** A REVIEW OF HATE CRIMES IN NEBRASKA

If you were to scrape all the data from each PDF, multiple data frames totaling an estimated 100 rows and 25 columns would be created.  The reports are created by the NCC in order to give an interested party a general overview of crime within the state, and how it compared to crime from the previous year.  This data will be combined with other sources to answer research questions 1 &amp; 3.  We identified the table &quot;Percent Change - Crime Index Offenses January thru December, 2015 - 2016 by Population Groups&quot; as the most relevant to our research questions.

Metadata:

As this is a published report and not a table in a database, all row and column titles were clear to understand. There is a textual description at the top of the report detailing where the information was collected from and how to interpret the results. Throughout the report descriptions of tables and footnotes are provided.  Additionally, reading about NCC provided context to how and why the data was collected.

Remediation Needed:

The years 2007-2017 needed to be scraped and combined into a single table. The table we chose to scrape included data the from past year, this would lead to duplicate entries in our final table.  It also included derived statistics, such as percent change, and crime category totals.  The format of the table in the report would have triple header column names, and double row indexes, these would be an additional pain point when scripting with the data.

**Offense Type by Age Group &amp; Offense Type by Year**

Citation:

&quot;Arrest Data Query&quot; 2017, http://www.nebraska.gov/crime\_commission/arrest/arrest.cgi. Accessed 18 Sept. 2017.

Description:

Using the Arrest Data Query database provided by the NCC, we ran 2 queries to obtain the above datasets.  The queries run on a full relation database we can not guess the size of.  Each result generated from a query returned a 30 x 15 table, the axis names of which are contained in the names of the data sources.  Their database engine only allows you to select 2 axis categories with which to execute queries, it does not allow for SQL querying.

Metadata:

The Nebraska Open Data project comes directly from the NE Governor&#39;s office.  There is a lot of information about State operations, but nothing directly addressing the Open Data initiative ( [http://www.nebraska.gov/government/open-data/](http://www.nebraska.gov/government/open-data/)).  On Arrest Data Query page, a paragraph details the database provided, and how to use it.

Remediation Needed:

As we were able to directly copy paste the values from the in-browser table generated from our query into Excel, no data cleaning was needed.  All column names were clear, and no nulls were present.  Any alterations would be aesthetic only.

**Offence Type by Month &amp; Offence Type by Year**

Citation:

&quot;Offence Data Query&quot;, [http://www.nebraska.gov/crime\_commission/arrest/offense.cgi](http://www.nebraska.gov/crime_commission/arrest/offense.cgi)

Description:

Offence data is based on the crimes known to the law enforcement. It is based on the arrest dataset i.e. a person is arrested for what type of offence committed? The offence data is collected from the following crimes: **Homicide, rape, assault, robbery, burglary, larceny, motor vehicle theft, and arson.**

Using the Offence Data Query database provided by the NCC, we ran two queries and a sub query to obtain the above datasets. Each result from the query generates 31 columns and 14 rows for a specific year.

Metadata:

The Nebraska Open Data project comes directly from the NE Governor&#39;s office.  There is a lot of information about State operations and the crime information is obtained from the Nebraska Crime Commission, but nothing directly addressing the Open Data initiative ( [http://www.nebraska.gov/government/open-data/](http://www.nebraska.gov/government/open-data/)).  On Offence Data Query page, a paragraph details the database provided, and how to use it.

Remediation Needed:

As we were able to directly copy paste the values from the in-browser table generated from our query into Excel, no data cleaning was needed.  All column names were clear, and no nulls were present.  Any alterations would be aesthetic only.

**S&P 500 Monthly Price**

Citation:

&quot;S&amp;P 500 Historical Prices by Month&quot; 2017, http://www.multpl.com/s-p-500-historical-prices/table/by-month. Accessed 1 Oct. 2017.

Description:

This data was contained in a 1752 x 2 table listing the price of the S&amp;P 500 on the 1st of the month, for each year going back to 1871. The site appears to be maintained by a stock enthusiast, possibly to drive ad traffic to the site.

Metadata:

N/A

Remediation Needed:

As we were able to directly copy paste the values from the in-browser table into Excel, no data cleaning was needed.  All column names were clear, and no nulls were present.  Any alterations would be aesthetic only.

**Unemployment Data across the state of Nebraska**
Citation:

[Nebraska Unemployment Statistics&quot; 2017](https://data.bls.gov/timeseries/LASST310000000000003) [https://data.bls.gov/timeseries/LASST310000000000003. Accessed 12 Oct. 2017.](https://data.bls.gov/timeseries/LASST310000000000003)

Description:

The Nebraska unemployment statistics is obtained from the Bureau of Labor Statistics. The unemployment database can be formatted by specifying the year range and the view. For our project, I have specified the year range as 2007-2016 and the dataset generated from that query consists of 130 rows and 6 columns. The columns are describes as &#39;Year&#39;, &#39;Period&#39;, &#39;Labor force&#39;, &#39;employment&#39;, &#39;unemployment&#39;, and &#39;unemployment rate&#39;.

Metadata:

The unemployment data is obtained from the Bureau of Labor Statistics of the U.S. Department ( [https://www.bls.gov/](https://www.bls.gov/)).  There is a lot of information about the labor market activity, working conditions, price changing conditions etc. Nebraska unemployment data can be obtained from the Local Area Unemployment Statistics ( [https://www.bls.gov/lau/home.htm](https://www.bls.gov/lau/home.htm)).

Remediation Needed:

As the dataset was already available in an excel format ready to be downloaded from the in-browser, no data cleaning was needed.  All column names were clear, and no nulls were present.  Any alterations would be aesthetic only.

**County Population**

Citation:

&quot;Nebraska Counties by Population&quot; 2017,

[https://www.nebraska-demographics.com/counties\_by\_population](https://www.nebraska-demographics.com/counties_by_population). Accessed 3 Oct. 2017.

Description: This is data is from US census and is for 92 Counties ranked 1 through 92.

Metadata: NA

Remediation Needed: The data was already very pretty and could be pasted to any worksheet. Although later some cleaning was done to successfully import it to the dataframe in python scripts.

**Data Cleaning Steps:**

Organized by research question and author:

 **1. How does a county's population level impact the types of crimes committed? - Shivani Singh Parihar**

=== County Crime and County Population Cleaning and Merging ===

= Documented by Shivani Singh

=== General Info ===

Manual cleaning steps are detailed in &quot;Cleaning Steps&quot;

All other cleaning procedures were completed in:

python 3.6.1

pandas 0.20.1

numpy 1.12.1

R Studio

Scripts are written in R with Python. Scripts have to be in the same folder where the raw data files are in order to be executed successfully

=== Cleaning Steps ===

\*\*\*All functions referenced for data cleaning can be found in &quot;crime\_and\_population\_cleaning.py&quot; and the dataframe merging steps can be found in &quot;Combining\_crime\_and\_population.R&quot;\*\*\*

Data Cleaning Steps in Python

\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

1. Data was scraped from the &quot;Law Enforcement Employment in Nebraska Series&quot; for the years 2016 through 2007 - (https://ncc.nebraska.gov/stat-reports#Crime\_in\_Nebraska\_Series) and by Nebraska demographics gave population for each county - (https://www.nebraska-demographics.com/counties\_by\_population)

Data Checkpoint: County\_and\_agency\_based\_crime.csv &amp; County\_population.csv

2. The county crime had data from 2000 onwards but we are only working with data from 2007 so the data before 2007 is removed

3. The crime data had Agency data for each county and it has to be dropped as well it would have prevented us from grouping(using GROUPBY) the county crime data

4. To complete our analysis, it was necessary to include the county&#39;s population as it would help us analyze how the population affects the number of crimes in each county. And in order to acheive this the county population column from county population data was merged with crime data

5. It was discovered that few spaces in County\_population.csv has some spaces which created problems while importing the file with panda. so in order to fix that I initially used sublime text and regex to replace those problematic spaces with an underscore

6. After importing(refer step 5) I replaced the \_County with and empty string in order to make a column of common values between the two CSVs that is County\_and\_agency\_based\_crime.csv &amp; County\_population.csv

7. Still few underscores persisted with in the county names, they were removed with the same approach as in step 6 that is they were replaced by using replace function and replaced underscore with an empty string

Data merging Steps in R

\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

The data merging step between two CSV was difficult in Python as compared to R. So R was a better option to opt at this stage

1. First, set the working directory where pre-cleaned files from python scripts are located

2. Create two dataframes for two different CSVs we are dealing with that is County\_and\_agency\_based\_crime.csv &amp; County\_population.csv

3. Create a new column &quot;Population&quot; in the crime data CSV and populate it with the matching county population data from the County\_population.csv. Here the common column between both the CSV is &quot;County&quot;

Final Cleaned Data: County\_crime\_and\_population\_combined.csv

 **2. How do police staffing levels impact overall Nebraska crime rates? - Will Wetzel**

=== Law Enforcement Employment and Crime by Agency Cleaning and Merging ===

= Documented by William Wetzel

=== General Info ===

Manual cleaning steps are detailed in &quot;Cleaning Steps&quot;

All python script cleaning procedures were completed in:

python 3.6.1

pandas 0.20.1

numpy 1.12.1

Scripts can be run in R with rPython.  You will still need to open the python file to select which function you are going to run, and specify file locations on your machine if needed.  The script is meant to be run as it was developed, one function at a time.  Even if running the script in R, you will need to manipulate the python code to point to the correct csv files, and to access the desired functions.

=== Cleaning Steps ===

\*\*\*All functions referenced for data cleaning can be found in &quot;law\_enforcement\_data\_cleaning.py&quot;\*\*\*

1. Data was scraped from the &quot;Law Enforcement Employment in Nebraska Series&quot; for the years 2016 through 2007 - (https://ncc.nebraska.gov/stat-reports#Crime\_in\_Nebraska\_Series)

    Only one table from each pdf was scraped, it was decided the statistics for &quot;Cities Above 5,000 Population&quot; would be the most relevant to our research.  An example of this table is found on page 4 of the 2016 employment data.

    Tabula (http://tabula.technology/) was used to convert each pdf table into a csv, which were then manually combined into a single cvs using excel using copy and paste functions.  After the first year of data was scraped, a new column at the leftmost position was inserted named &quot;year&quot;, when a year of data was pasted into the combined table, the corresponding year was entered in the year column for all departments.  Now &quot;year&quot; and &quot;department&quot; created a composite primary key for the table.

Once all years of data had been scraped and combined, column titles were changed to reflect those detailed in &quot;dataDict\_lawStaff\_vs\_Crime.txt&quot;, each column name was essentially created into an abbreviation.

Data Checkpoint: raw\_combined\_data\_law\_enforcement\_2007-2017.csv

2. To complete our analysis, it was necessary to include the county in which each department is located.  A new column for county was added in the combined dataset using excel, it was inserted as the third column after &quot;department&quot;.  For the year 2007, counties were researched and input by hand for each department.  I would type the department name into google, find the county, and enter it with the corresponding department in 2007.

3. It was discovered during this time, department names had different formats for each dataset year.  For example, in 2007 department would &quot;Omaha P.D&quot; and in 2014 it would be &quot;OMAHA PD&quot;.  To standardize department names, names were manually replaced in excel with copy paste by using the 2007 department naming convention for all years.  It was ensured that for each year, the number of departments and their order were identical prior to copy pasting.

Data Checkpoint: law\_enforcement\_2007-2017\_department\_names\_matching.csv

4. Now that all department names for each year matched, the function &quot;law\_enforcement\_county\_copier&quot; from &quot;law\_enforcement\_data\_cleaning.py&quot; was used to copy department counties to all departments.

Data Checkpoint: &quot;law\_enforcement\_2007-2017\_counties.csv&quot;

5. The function &quot;period\_remover&quot; from &quot;law\_enforcement\_data\_cleaning.py&quot; was used to turn department names from &quot;… P.D.&quot; to &quot;… PD&quot;

Data Checkpoint: law\_enforcement\_2007-2017\_pd.csv

6. The function &quot;department\_merger&quot; was used to first identify departments from &quot;law\_enforcement\_2007-2017\_counties.csv&quot; which did not have crime statistics in &quot;All\_Crimes\_by\_Year\_and\_Agency.csv&quot;, data cleaning steps were included to rectify 2 missing names.

Then the crime data was merged by agency or department with the law\_enforcement\_employment data.

Final Cleaned Data: lawStaff\_vs\_Crime.csv

**3. What trends in the types and amount of crimes committed can be observed? - Chaitra Venkatesan**

=== Offence Type and Offence Month by Year Cleaning and Merging ===

= Documented by Chaitra Venkatesan

=== General Info ===

Manual cleaning steps are detailed in &quot;Cleaning Steps&quot;

=== Cleaning Steps ===

1.  Data was scraped from the Offence Data Query hosted on the Nebraska government website ( [http://www.nebraska.gov/crime\_commission/arrest/offense.cgi](http://www.nebraska.gov/crime_commission/arrest/offense.cgi)) for each particular year, starting from 2007 through 2016. The dataset generated was compiled for each year by having the offence type as column as months as rows.

Data Checkpoint: Offence\_Month\_Year\_NE\_2007-2016.csv

2. The offence type for offence month can be obtained from the offence data query by specifying the year in the optional field.

3. For further analysis, it was necessary to obtain the unemployment data of Nebraska from 2007-2016. This data was obtained from the Bureau of Labor Statistics by specifying the year range and the generated dataset was available to download in a csv format.  The unemployment rate plays an important role in the number of offence committed.

Data Checkpoint: NE\_Unemployment\_Rate\_2007-2016.csv

4. To complete our analysis, we wanted to assess the S&amp;P 500 (Standard &amp; Poor) price change for each particular month from 2007-2016 and its effect on the offence committed. Historical monthly S&amp;P price data was scraped from ( [http://www.multpl.com/s-p-500-historical-prices/table/by-month](https://urldefense.proofpoint.com/v2/url?u=http-3A__www.multpl.com_s-2Dp-2D500-2Dhistorical-2Dprices_table_by-2Dmonth&amp;d=DwMFaQ&amp;c=kj4KSSBt5oOaHuCCpWXkZw&amp;r=3QlgnfZB0OtTC1ww9qrZOBFg2gN5hkX7qHnyB3EpEdc&amp;m=UH8YviaGs2QMmKNRjtSqXhDcKL5O6Eg9iCIpm-zbqQ0&amp;s=Zm8f17rc63OFbjfWmMiAZa5aeb-yBmGbjMEFpKMLp0c&amp;e=)), and copied into Excel for the dates of from Jan-2016 through Dec-2007.

5. A row was inserted at the top of the table to provide header columns.  For columns 1-4 of the table on the first row, column header names were entered - &quot;Month\_Year&quot;, &quot;S&amp;P500\_price&quot;, &quot;S&amp;P500\_price\_change&quot;, &quot;S&amp;P\_500\_percent\_change&quot;.

6. In the third column, a formula to subtract the earlier month from the later month was entered to calculate the price difference from month to month; this formula was copied for each month of data.  In the fourth column a formula of (Price Change/Previous Month Price) was entered to calculate the percent change month to month; this formula was applied to each month.

7. Rounding was set to 3 decimal places for the percent change column. A date format mask of &quot;mm/dd/yy&quot; was applied to the Month\_Year column.

Data Checkpoint: S&amp;P500\_Data\_2007-2016.csv

8. The columns obtained from the Offence\_Month\_Year\_NE\_2007-2016.csv were formatted accordingly:

·         Month\_Year was date formatted with a mask of &#39;mm-yy&#39;.

·         The offence type columns were formatted as Crime\_offence name

The rows were named from Jan-07 to Dec-16 and the corresponding offence data was copied from the Offence\_Month\_Year\_NE\_2007-2016.csv and merged accordingly.

9. Three new columns were inserted after the Month\_Year column and they are &#39;NE\_Unemployment\_Rate&#39; from NE\_Unemployment\_Rate\_2007-2016.csv and &#39;S&amp;P\_500\_Price\_Change&#39;, &#39;S&amp;P\_500\_Percent\_Gain\_or Loss&#39; from S&amp;P500\_Data\_2007-2016.csv.

10. The &#39;NE\_Unemployment\_Rate&#39; data from the NE\_Unemployment\_Rate\_2007-2016.csv and the

&#39;S&amp;P\_500\_Price\_Change&#39;, &#39;S&amp;P\_500\_Percent\_Gain\_or Loss&#39; data from the S&amp;P500\_Data\_2007-2016.csv were merged in the new combined sheet accordingly.

Final Cleaned Data: Off typ\_Off mnth\_Unemp\_S&amp;P500.csv

Through this compiled dataset, we can identify multiple patterns such as the impact of unemployment rate on the crimes committed, the impact of stock price change on the crimes committed etc. across the state of Nebraska.

### Data Cleaning Code
See Readme for further details on Data Cleaning code.  

#### County Population and Crime Data set

```{r eval=FALSE}
# Set the working directory to where you have your CSVs placed

setwd('/Users/shivanisingh/python_Datacleaning')

# Create a dataframe by reading the CSVs
t1 = read.csv('County_and_agency_based_crime.csv')
t2 = read.csv('County_population.csv')

# The next line creates a new column 'Population' in table t1 which is 'County_and_agency_based_crime.csv' and populates it with the 'Population'
# column of table t2 which is 'County_population.csv'.
# It maps the value for each column by matching the counties in both the table(it takes 'County' column as a key and matches the value)
t1$Population <- t2$Population[match(t1$County, t2$County)]

# The below line writes the merged CSV to County_crime_and_population_combined.csv
write.csv(t1, 'County_crime_and_population_combined.csv')
```

```{r eval=FALSE}
import pandas as pd 
import numpy as np 


def county_and_agency_crime(crime):
	df = pd.read_csv('County_and_agency_based_crime.csv')
	
	# removes columns before year 2007 and drop the Agency column

	colums_to_remove = [1,2,3,4,5,6,7,8]
	df.drop(df.columns[colums_to_remove], axis=1, inplace=True)
	columns_to_group = [0]

	#Group by the crime by eah County
	df_grouped_by_county = df.groupby('County').sum()
	print(df_grouped_by_county)

	#write the cleaned data to the 'County_and_agency_based_crime.csv'
	df_grouped_by_county.to_csv('County_and_agency_based_crime.csv')


# formats the raw csv in a way so that it can be compiled with the other csv

def county_population_cleaning(population):

	#used sublime regex to remove the spaces and replace it with '_'
	#used (?<!p)\h+ and replace with '_'
	
	df1 = pd.read_csv('County_population.csv')

	# Replaces the '_County' string from all the County values in order to have consistency through the data being handled and used

	df1['County'].replace('_County','',regex=True, inplace=True)

	# Replaces the '_' with an empty string

	df1['County'].replace('_',' ', regex=True, inplace=True)


	# Writes the data to the 'County_population.csv'
	df1.to_csv('County_population.csv')

# Calling the two data cleaning function

county_and_agency_crime('~/python_Datacleaning/County_and_agency_based_crime.csv')
county_population_cleaning('~/python_Datacleaning/County_population.csv')

```

#### Law Enforcement Staffing Rates

```{r eval=FALSE}
import pandas as pd
import numpy as np

# Used to propagate counties for each department throughout the dataset
#####################################
### law_enforcement_county_copier ###
#####################################

def law_enforcement_county_copier():
    df = pd.read_csv('law_enforcement_2007-2017.csv')

    nRows = df.shape[0]

    counties = {}

    for row in range(nRows):
        if  pd.notnull(df.loc[row, 'county']):
            counties[df.loc[row, 'department']] = df.loc[row, 'county']

    for row in range(nRows):
        if df.loc[row, 'department'] in counties:
            df.loc[row, 'county'] = counties[df.loc[row, 'department']]

    df.to_csv('law_enforcement_COUNTIES_2007-2017.csv', index=False)
    
# Used to turn "P.D." to "PD"
######################
### period_remover ###
######################

def period_remover(csv):
    df = pd.read_csv(csv)

    nRows = df.shape[0]

    df['department'] = df['department'].str.replace('.', '')
    df.to_csv('law_enforcement_2007-2017_pd.csv', index=False)
    

# Used to find departments present in "law_enforcement_2007-2017.csv" but not "All_Crimes_by_Year_and_Agency.csv"
# Used to merge department crime data by year with employment statistics by year
###########################
### department_merger ###
###########################

def department_merger(law, crime):
    
    dfLaw = pd.read_csv(law)
    dfCrime = pd.read_table(crime, skiprows=[1,2])
    
    # Making a few missing department names match
    dfCrime['Agency'] = dfCrime['Agency'].str.replace('.', '')
    dfCrime['Agency'] = dfCrime['Agency'].str.replace('South Sioux City PD', 'So Sioux City PD')
    dfCrime['Agency'] = dfCrime['Agency'].str.replace('La Vista PD', 'LaVista PD')

    # Checking if all departments between the 2 csvs match
    departmentResults = dfLaw['department'].isin(dfCrime['Agency'])
    
    unmatchedDepartments = []
    for row in range(departmentResults.size):
        if departmentResults[row] == False:
            unmatchedDepartments.append(dfLaw.loc[row, 'department'])    
    # Get unique names of departments which don't match        
    unmatchedDepartments = set(unmatchedDepartments)
        
    # Merging the 2 dataframes on department and year
    lawCrimeMerged = dfLaw
    years = ['2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016']
    
    # Initialize a new column for total crime
    lawCrimeMerged['total_crime'] = np.nan
    # For each 
    for row in range(lawCrimeMerged.shape[0]):
        tempDataFrame = (dfCrime.loc[dfCrime['Agency'] == lawCrimeMerged.loc[row, 'department']])
        for year in years:
            number = int(tempDataFrame[year])
            index = lawCrimeMerged.index[(lawCrimeMerged['department'] == lawCrimeMerged.loc[row, 'department']) & (lawCrimeMerged['year'] == int(year))].tolist()
            lawCrimeMerged.loc[index, 'total_crime'] = number            

    lawCrimeMerged.to_csv('lawCrimeMerged.csv', index=False)


# Run individual functions here

#law_enforcement_county_copier():   
#period_remover('law_enforcement_2007-2017.csv')
#department_merger('law_enforcement_2007-2017.csv', 'All_Crimes_by_Year_and_Agency.txt')
```

#### Unemployment Level and Crime

Please see Excel cleaning steps detailed above in the writeup.

## Analysis Scripts
#### How do police staffing levels impact overall crime rates?
##### Dataset: Law Enforcement Staffing Rates

```{r include=FALSE}
library(data.table)
# Get data from our Github
lawCrime <- fread('https://raw.githubusercontent.com/Shivani-Parihar/Data-Rangers/master/Deliverables/DataCleaningDeliverable/clean_data/lawStaff_vs_Crime.csv', showProgress = FALSE)
```

```{r echo=FALSE, include=FALSE}
# Replace null values with 0, from inconsistent data cleaning
columnsToReplace <- lawCrime[,c("us_so_ft", "us_so_pt", "num_res_so",
                                "us_c_ft", "us_c_pt")]
columnsToReplace[is.na(columnsToReplace)] <- 0
lawCrime[,c("us_so_ft", "us_so_pt", "num_res_so",
            "us_c_ft", "us_c_pt")] <- columnsToReplace

# Checking for rows with nulls,
# if nothings prints the data has no nulls
nulls <- complete.cases(lawCrime)
nulls
counter <- 0
for (null in nulls) {
  if (!null){
    print(paste("You have nulls at row", counter))
  }
  counter <- counter+1
}

# Create a total full time officer column,
# combining male and female records
lawCrime$total_so <- (lawCrime$ft_so_m + lawCrime$ft_so_f)
# Same for full time civilians
lawCrime$total_c <- lawCrime$ft_c_m + lawCrime$ft_c_f

# Convert pop_covered from chr to num
lawCrime$pop_covered <- as.numeric(gsub(",", "", lawCrime$pop_covered))
str(lawCrime)

# Create a crime per 1000 column
lawCrime$crime_per_1000 <- lawCrime$total_crime / lawCrime$pop_covered

# Creating a sworn officer employment change amount from year to year for each department
lawCrime$total_so_change <- 0
lawCrime$total_Fso_change <- 0
lawCrime$total_Mso_change <- 0

departments <- unique(lawCrime$department)
years <- unique(lawCrime$year)
# Remove 2016
years <- years[-10]

# Starting at 2008, record the change in total_so from the previous year for all departments
for (department in departments){
  for (year in years){
    tryCatch({
  row <- which(lawCrime$department == department & lawCrime$year == year+1)
  lawCrime[row,21] <- lawCrime$total_so[lawCrime$department == department & lawCrime$year == year+1] - lawCrime$total_so[lawCrime$department == department & lawCrime$year == year]}, error=function(e){})
  }
}
# Starting at 2008, record the change in total_so from the previous year for female officers
for (department in departments){
  for (year in years){
    tryCatch({
  row <- which(lawCrime$department == department & lawCrime$year == year+1)
  lawCrime[row,22] <- lawCrime$ft_so_f[lawCrime$department == department & lawCrime$year == year+1] - lawCrime$ft_so_f[lawCrime$department == department & lawCrime$year == year]}, error=function(e){})
  }
}
# Starting at 2008, record the change in total_so from the previous year for male officers
for (department in departments){
  for (year in years){
    tryCatch({
  row <- which(lawCrime$department == department & lawCrime$year == year+1)
  lawCrime[row,23] <- lawCrime$ft_so_m[lawCrime$department == department & lawCrime$year == year+1] - lawCrime$ft_so_m[lawCrime$department == department & lawCrime$year == year]}, error=function(e){})
  }
}

```

**Checking for any correlation between variables and the relative crime rate per 1000**
The Crime rate per 1000 people is used as our starting point.  It is important to have a relative measure of crime with which to compare as the more highly populated an area - the more total crime we would expect, and we would expect a more populated area to have more total officers.
```{r}
pCor_pop_covered <- cor(lawCrime$pop_covered, lawCrime$crime_per_1000, method="pearson")
pCor_pop_covered
pCor_so_per_1000 <- cor(lawCrime$so_per_1000, lawCrime$crime_per_1000, method="pearson")
pCor_so_per_1000
pCor_total_so <- cor(lawCrime$total_so, lawCrime$crime_per_1000, method="pearson")
pCor_total_so
pCor_annualChange <- cor(lawCrime$total_so_change, lawCrime$crime_per_1000, method="pearson")
pCor_annualChange
pCor_annualFChange <- cor(lawCrime$total_Fso_change, lawCrime$crime_per_1000, method="pearson")
pCor_annualFChange
pCor_annualMChange <- cor(lawCrime$total_Mso_change, lawCrime$crime_per_1000, method="pearson")
pCor_annualMChange
```
Interestingly, we find that the ratio of officers to civilians and the annual change in the hiring of officers has little impact on the overall crime rate.  It makes sense that the total number of sworn officers, and the total population covered by a department are moderately positively correlated with overall crime rate.  The more urban the setting - we find that there is more overall crime.

**Looking for highest rates of crime per 1000 by department**
For 2007-2016 for all departments, we sort by the highest crime rate per 1000 and select the top 30.  We can see that for 305 distinct crime rate measurements 8 departments share the worst 10% of crime rates.  In theory, since this is a relative measure - each county could be equally represented.  Scottsbluff and Grand Island are perhaps surprising that they showed up as many times as they did.  Omaha even though it is by far the most populous area only accounts for 4/30 of the worst 30 crime measurements in the dataset.
```{r}
# Sort by crime rate per 1000
lawCrimeSortCrimeRate <- lawCrime[order(-lawCrime$crime_per_1000),]
lawCrimeSortCrimeRate <- head(lawCrimeSortCrimeRate, 30)
worstCrimeRates <- table(lawCrimeSortCrimeRate$department)
worstCrimeRates
```
**Plotting the 2 variables which show some positive correlation with the crime rate per 1000**
My analysis of these plots is that it is largely the urban areas which account for the positive correlation.  Our conclusion is that at least in Nebraska, your relative crime rate will increase as you move to more populous areas of the state.  
```{r}
library("ggpubr")
ggscatter(lawCrime, x = "pop_covered", y = "crime_per_1000", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Total Population Covered", ylab = "Crime per 1000", main = "Population Covered and Crime Rate")
```

```{r}
ggscatter(lawCrime, x = "total_so", y = "crime_per_1000", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Total Sworn Officers", ylab = "Crime per 1000", main = "Total Sworn Officers and Crime Rate")

```
```{r include=FALSE}

years <- unique(lawCrime$year)
totAvg <- NA
fAvg <- NA
mAvg <- NA
cAvg <- NA

for (year in years){
  totAvg <- append(totAvg, mean(lawCrime$total_so[lawCrime$year == year]))
  fAvg <- append(fAvg, mean(lawCrime$ft_so_f[lawCrime$year == year]))
  mAvg <- append(mAvg, mean(lawCrime$ft_so_m[lawCrime$year == year]))
  cAvg <- append(cAvg, mean(lawCrime$total_crime[lawCrime$year == year]))

}
totAvg <- totAvg[-1]
fAvg <- fAvg[-1]
mAvg <- mAvg[-1]
cAvg <- cAvg[-1]
hiringCrimePlot <- data.frame(years, totAvg, fAvg, mAvg, cAvg)
hiringCrimePlot$cAvg <- hiringCrimePlot$cAvg / 40
```

```{r}
library(ggplot2)

p <- ggplot(hiringCrimePlot, aes(x=years, y=cAvg)) +
  geom_point(aes(x = years, y = totAvg, color = "Total Officers")) + 
  geom_point(aes(x = years, y = fAvg, color = "Total Female Officers")) + 
  geom_point(aes(x = years, y = mAvg, color = "Total Male Officers")) + 
  geom_line(aes(x = years, y = cAvg, color = "Crime Trend (Not Actual Figures)"))
p <- p + scale_fill_discrete(name = "New Legend Title")
p <- p + labs(x = "Year", y = "Number", title = "Police Staffing Impact on Crime Rate")
p <- p + theme(legend.title=element_blank())
p
```
  
When we compare average hiring data year over year, it isn't obvious there is any relationship between the hiring rates of officers and the overall incidents of crime across the state.  In fact the year with the lowest crime incidence rate (2015), led to the greatest hiring spike, indicating at time this can even be an inverse relationship.

### Looking at the data geographically

```{r include=FALSE}

# Mapping county zip codes to names
library(data.table)
postalCodes <- fread('https://raw.githubusercontent.com/Shivani-Parihar/Data-Rangers/master/Deliverables/R_Plot_Deliverable/us_postal_codes.csv')
colnames(postalCodes)[5] <- "county"

# Just keep zip codes and county names, and states
postalCodes <- postalCodes[,c(-2,-4,-6,-7,-8)]

# Rows in dataset matching counties in lawCrime and located in NE
indexes <- which(postalCodes$State == 'Nebraska' & postalCodes$county %in% lawCrime$county) #& ])
# Keep only matching rows in the zipcode file
postalCodes <- postalCodes[indexes,]
# Keep only one zip for each County
postalCodes <- postalCodes[!duplicated(postalCodes$county),]


# Create a new dataframe merging LawCrime with county zip codes
lawCrimeMapping <- merge(lawCrime, postalCodes, by = "county", type = "inner")

# Remove state and department columns
lawCrimeMapping <- lawCrimeMapping[,-c(21, 3)]


#!!!!! Map only 2016 data !!!!!#
lawCrimeMapping <- lawCrimeMapping[lawCrimeMapping$year == 2016,]
# Aggregate a number of columns in the event you want to map different things
lawCrimeMapping$county <- as.character(lawCrimeMapping$county)

lawCrimeMapping$pop_covered <- as.numeric(lawCrimeMapping$pop_covered)

lawCrimeMapping <- aggregate(lawCrimeMapping[,c(3,4,5,6,7,8,9,10,12,13,14,17,18)], 
                             by=list(county=lawCrimeMapping$county), 
                             FUN=sum)
```

```{r}
# Calling mapping libraries
library(readxl)
library(dplyr)
library(choroplethr)
library(choroplethrMaps)
library(ggplot2)
library(maps)
# Assigning a "region" to each county, 
# "region" is a ggplot specific mapping address
data(county.map)
# Keep Nebraska data only
county.map <- filter(county.map, STATE == 31)
# Keep only one region code per county name
county.map <- county.map[!duplicated(county.map$NAME),]
# Join by county name
lawCrimeMapping <- left_join(lawCrimeMapping, county.map, by = c("county" = "NAME"))
# value is a required mapping column name
colnames(lawCrimeMapping)[13] <- "value"
# Plotting by number of full time officers
county_mapSo <- county_choropleth(lawCrimeMapping, legend = "Number of Sworn Officers, 2016", 
                                state_zoom = "nebraska", num_colors=6)
# We can see there are many counties not officially covered by any police department
```

```{r, echo = FALSE}
county_mapSo
```
We can see that the actual number of sworn officers differs by regions of the state.  Also, most counties do not have a police department located within their borders.  Our data doesn't indicate how police resources are allocated across county lines.


```{r, include=FALSE}
# Plotting by SO per 1000
SoPer1000 <- lawCrimeMapping[,c("region", "so_per_1000")]
colnames(SoPer1000)[2] <- "value"
county_SoPer1000 <- county_choropleth(SoPer1000, legend = "Number of Sworn Officers Per 1000, 2016", 
                                   state_zoom = "nebraska", num_colors=6)
# It makes sense the central part of the state, being the most spread out,
# has the lowest ratio
```

```{r, echo=FALSE}
county_SoPer1000
```
Here we can see that some of the most populous areas, like Omaha and Grand Island, have some of the best sworn officer to general population ratio - but still experience the highest crime rate per 1000 people.  This leads us to conclude that more police officers will not necessarily reduce the amount of crime in a given county.

#### Conclusions:
#### How do police staffing levels impact overall crime rates?

After thoroughly reviewing our data there are some key conclusions we can make.  

1. Population density is the primary and only factor we discovered which statistically impacts crime rate.
  *While it is not a direct relationship that the more population density present the more crime rate present, all of the counties with the highest crime rate fall within the top 10% of counties by population. While Omaha doesn't necessarily have more crime than Lincoln, an community with a population greater than 25,000 is significantly more likely to experience more crime per 1000 people than a rual community.  We have discovered that according to our data, you could hiring more officers in urban communities and it would not impact the crime rate.  We know this - as rural communites have the same or lower officer to population ratio, and experience lower levels in crime than urban areas.

2. Hiring trends do not appear to be tied to the crime rate in any meaningful way.
  *By plotting hiring and crime data over time, we can see there is no obvious relationship between the reported crime rate and hiring of police officers.


  